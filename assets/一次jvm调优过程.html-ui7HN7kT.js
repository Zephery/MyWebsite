import{_ as s,c as a,d as e,o as t}from"./app-BvzaS-vR.js";const h={};function n(p,i){return t(),a("div",null,[...i[0]||(i[0]=[e(`<h1 id="一次jvm调优过程" tabindex="-1"><a class="header-anchor" href="#一次jvm调优过程"><span>一次jvm调优过程</span></a></h1><p>前端时间把公司的一个分布式定时调度的系统弄上了容器云，部署在kubernetes，在容器运行的动不动就出现问题，特别容易jvm溢出，导致程序不可用，终端无法进入，日志一直在刷错误，kubernetes也没有将该容器自动重启。业务方基本每天都在反馈task不稳定，后续就协助接手看了下，先主要讲下该程序的架构吧。 该程序task主要分为三个模块： console进行一些cron的配置（表达式、任务名称、任务组等）； schedule主要从数据库中读取配置然后装载到quartz再然后进行命令下发； client接收任务执行，然后向schedule返回运行的信息（成功、失败原因等）。 整体架构跟github上开源的xxl-job类似，也可以参考一下。</p><h2 id="_1-启用jmx和远程debug模式" tabindex="-1"><a class="header-anchor" href="#_1-启用jmx和远程debug模式"><span>1. 启用jmx和远程debug模式</span></a></h2><p>容器的网络使用了BGP，打通了公司的内网，所以可以直接通过ip来进行程序的调试，主要是在启动的jvm参数中添加：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">JAVA_DEBUG_OPTS</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot; -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,address=0.0.0.0:8000,server=y,suspend=n &quot;</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">JAVA_JMX_OPTS</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot; -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false &quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，调试模式的address最好加上0.0.0.0，有时候通过netstat查看端口的时候，该位置显示为127.0.0.1，导致无法正常debug，开启了jmx之后，可以初步观察堆内存的情况。</p><p><img src="https://github-images.wenzhihuai.com/images/20200121102100646998927.png" alt="" loading="lazy"></p><p><img src="https://github-images.wenzhihuai.com/images/20200121102112646209333.png" alt="" loading="lazy"></p><p>堆内存（特别是cms的old gen），初步看代码觉得是由于用了大量的map，本地缓存了大量数据，怀疑是每次定时调度的信息都进行了保存。</p><h2 id="_2-memory-analyzer、jprofiler进行堆内存分析" tabindex="-1"><a class="header-anchor" href="#_2-memory-analyzer、jprofiler进行堆内存分析"><span>2. memory analyzer、jprofiler进行堆内存分析</span></a></h2><p>先从容器中dump出堆内存</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">jmap</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -dump:live,format=b,file=heap.hprof</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 58</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://github-images.wenzhihuai.com/images/202001211021411245129253.png" alt="" loading="lazy"></p><p>由图片可以看出，这些大对象不过也就10M，并没有想象中的那么大，所以并不是大对象的问题，后续继续看了下代码，虽然每次请求都会把信息放进map里，如果能正常调通的话，就会移除map中保存的记录，由于是测试环境，执行端很多时候都没有正常运行，甚至说业务方关闭了程序，导致调度一直出现问题，所以map的只会保留大量的错误请求。不过相对于该程序的堆内存来说，不是主要问题。</p><h2 id="_3-netty的方面的考虑" tabindex="-1"><a class="header-anchor" href="#_3-netty的方面的考虑"><span>3. netty的方面的考虑</span></a></h2><p>另一个小伙伴一直怀疑的是netty这一块有错误，着重看了下。该程序用netty自己实现了一套rpc，调度端每次进行命令下发的时候都会通过netty的rpc来进行通信，整个过程逻辑写的很混乱，下面开始排查。 首先是查看堆内存的中占比：</p><p><img src="https://github-images.wenzhihuai.com/images/20200121102201592637921.png" alt="" loading="lazy"></p><p>可以看出，io.netty.channel.nio.NioEventLoop的占比达到了40%左右，再然后是io.netty.buffer.PoolThreadCache，占比大概达到33%左右。猜想可能是传输的channel没有关闭，还是NioEventLoop没有关闭。再跑去看一下jmx的线程数：</p><p><img src="https://github-images.wenzhihuai.com/images/202001211022191867685117.png" alt="" loading="lazy"></p><p>达到了惊人的1000个左右，而且一直在增长，没有过下降的趋势，再次猜想到可能是NioEventLoop没有关闭，在代码中全局搜索NioEventLoop，找到一处比较可疑的地方。</p><p><img src="https://github-images.wenzhihuai.com/images/20200121102239634803742.png" alt="" loading="lazy"></p><p>声明了一个NioEventLoopGroup的成员变量，通过构造方法进行了初始化，但是在执行syncRequest完之后并没有进行对group进行shutdownGracefully操作，外面对其的操作并没有对该类的group对象进行关闭，导致线程数一直在增长。</p><p><img src="https://github-images.wenzhihuai.com/images/20200121102251548240533.png" alt="" loading="lazy"></p><p>最终解决办法： 在调用完syncRequest方法时，对ChannelBootStrap的group对象进行行shutdownGracefully</p><p><img src="https://github-images.wenzhihuai.com/images/202001211023001796761285.png" alt="" loading="lazy"></p><p>提交代码，容器中继续测试，可以基本看出，线程基本处于稳定状态，并不会出现一直增长的情况了</p><p><img src="https://github-images.wenzhihuai.com/images/202001211023141978405713.png" alt="" loading="lazy"></p><p>还原本以为基本解决了，到最后还是发现，堆内存还算稳定，但是，直接内存依旧打到了100%，虽然程序没有挂掉，所以，上面做的，可能仅仅是为这个程序续命了而已，感觉并没有彻底解决掉问题。</p><p><img src="https://github-images.wenzhihuai.com/images/202001211023231033694109.png" alt="" loading="lazy"></p><h2 id="_4-直接内存排查" tabindex="-1"><a class="header-anchor" href="#_4-直接内存排查"><span>4. 直接内存排查</span></a></h2><p>第一个想到的就是netty的直接内存，关掉，命令如下：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">-Dio.netty.noPreferDirect</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">true</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -Dio.netty.leakDetectionLevel=advanced</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://github-images.wenzhihuai.com/images/202001211023381459531529.png" alt="" loading="lazy"></p><p>查看了一下java的nio直接内存，发现也就几十kb，然而直接内存还是慢慢往上涨。毫无头绪，然后开始了自己的从linux层面开始排查问题</p><h2 id="_5-推荐的直接内存排查方法" tabindex="-1"><a class="header-anchor" href="#_5-推荐的直接内存排查方法"><span>5. 推荐的直接内存排查方法</span></a></h2><h4 id="_5-1-pmap" tabindex="-1"><a class="header-anchor" href="#_5-1-pmap"><span>5.1 pmap</span></a></h4><p>一般配合pmap使用，从内核中读取内存块，然后使用views 内存块来判断错误，我简单试了下，乱码，都是二进制的东西，看不出所以然来。</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pmap</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -d</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 58</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  | </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sort</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -n</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -k2</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pmap</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -x</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 58</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  | </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sort</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -n</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -k3</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">grep</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> rw-p</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /proc/</span><span style="--shiki-light:#383A42;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">$1</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">/maps</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> | </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sed</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;s/^\\([0-9a-f]*\\)-\\([0-9a-f]*\\) .*$/\\1 \\2/p&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> | </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">while</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> read</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> start</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> stop</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">; </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">do</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> gdb</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --batch</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --pid</span><span style="--shiki-light:#383A42;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;"> $1</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -ex</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;dump memory </span><span style="--shiki-light:#383A42;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">$1</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">-</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">$start</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">-</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">$stop</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">.dump 0x</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">$start</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 0x</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">$stop</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">; </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">done</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这个时候真的懵了，不知道从何入手了，难道是linux操作系统方面的问题？</p><h4 id="_5-2-gperftools-https-github-com-gperftools-gperftools" tabindex="-1"><a class="header-anchor" href="#_5-2-gperftools-https-github-com-gperftools-gperftools"><span>5.2 [gperftools]（https://github.com/gperftools/gperftools）</span></a></h4><p>起初，在网上看到有人说是因为linux自带的glibc版本太低了，导致的内存溢出，考虑一下。初步觉得也可能是因为这个问题，所以开始慢慢排查。oracle官方有一个jemalloc用来替换linux自带的，谷歌那边也有一个tcmalloc，据说性能比glibc、jemalloc都强，开始换一下。 根据网上说的，在容器里装libunwind，然后再装perf-tools，然后各种捣鼓，到最后发现，执行不了，</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pprof</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --text</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /usr/bin/java</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> java_58.0001.heap</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://github-images.wenzhihuai.com/images/202001211024021477267177.png" alt="" loading="lazy"></p><p>看着工具高大上的，似乎能找出linux的调用栈，</p><h2 id="_6-意外的结果" tabindex="-1"><a class="header-anchor" href="#_6-意外的结果"><span>6. 意外的结果</span></a></h2><p>毫无头绪的时候，回想到了linux的top命令以及日志情况，测试环境是由于太多执行端业务方都没有维护，导致调度系统一直会出错，一出错就会导致大量刷错误日志，平均一天一个容器大概就有3G的日志，cron一旦到准点，就会有大量的任务要同时执行，而且容器中是做了对io的限制，磁盘也限制为10G，导致大量的日志都堆积在buff/cache里面，最终直接内存一直在涨，这个时候，系统不会挂，但是先会一直显示内存使用率达到100%。 修复后的结果如下图所示：</p><p><img src="https://github-images.wenzhihuai.com/images/202001211024142482541.png" alt="" loading="lazy"></p><p><img src="https://github-images.wenzhihuai.com/images/202001211024261078778632.png" alt="" loading="lazy"></p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>定时调度这个系统当时并没有考虑到公司的系统会用的这么多，设计的时候也仅仅是为了实现上千的量，没想到到最后变成了一天的调度都有几百万次。最初那批开发也就使用了大量的本地缓存map来临时存储数据，然后面向简历编程各种用netty自己实现了通信的方式，一堆坑都留给了后人。目前也算是解决掉了一个由于线程过多导致系统不可用的情况而已，但是由于存在大量的map，系统还是得偶尔重启一下比较好。</p><p>参考：<br> 1.<a href="https://www.cnblogs.com/testfan2019/p/11151008.html" target="_blank" rel="noopener noreferrer">记一次线上内存泄漏问题的排查过程</a><br> 2.<a href="https://coldwalker.com/2018/08//troubleshooter_native_memory_increase/" target="_blank" rel="noopener noreferrer">Java堆外内存增长问题排查Case</a><br> 3.<a href="https://static.rainfocus.com/oracle/oow18/sess/1524505564465001W0mS/PF/Troubleshooting_native_memory_leaks_1540301908390001k6DR.pdf" target="_blank" rel="noopener noreferrer">Troubleshooting Native Memory Leaks in Java Applications</a></p>`,51)])])}const r=s(h,[["render",n]]),g=JSON.parse('{"path":"/java/JVM/%E4%B8%80%E6%AC%A1jvm%E8%B0%83%E4%BC%98%E8%BF%87%E7%A8%8B.html","title":"一次jvm调优过程","lang":"zh-CN","frontmatter":{"description":"一次jvm调优过程 前端时间把公司的一个分布式定时调度的系统弄上了容器云，部署在kubernetes，在容器运行的动不动就出现问题，特别容易jvm溢出，导致程序不可用，终端无法进入，日志一直在刷错误，kubernetes也没有将该容器自动重启。业务方基本每天都在反馈task不稳定，后续就协助接手看了下，先主要讲下该程序的架构吧。 该程序task主要分为...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"一次jvm调优过程\\",\\"image\\":[\\"https://github-images.wenzhihuai.com/images/20200121102100646998927.png\\",\\"https://github-images.wenzhihuai.com/images/20200121102112646209333.png\\",\\"https://github-images.wenzhihuai.com/images/202001211021411245129253.png\\",\\"https://github-images.wenzhihuai.com/images/20200121102201592637921.png\\",\\"https://github-images.wenzhihuai.com/images/202001211022191867685117.png\\",\\"https://github-images.wenzhihuai.com/images/20200121102239634803742.png\\",\\"https://github-images.wenzhihuai.com/images/20200121102251548240533.png\\",\\"https://github-images.wenzhihuai.com/images/202001211023001796761285.png\\",\\"https://github-images.wenzhihuai.com/images/202001211023141978405713.png\\",\\"https://github-images.wenzhihuai.com/images/202001211023231033694109.png\\",\\"https://github-images.wenzhihuai.com/images/202001211023381459531529.png\\",\\"https://github-images.wenzhihuai.com/images/202001211024021477267177.png\\",\\"https://github-images.wenzhihuai.com/images/202001211024142482541.png\\",\\"https://github-images.wenzhihuai.com/images/202001211024261078778632.png\\"],\\"dateModified\\":\\"2025-11-01T04:49:29.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Zephery\\",\\"url\\":\\"https://wenzhihuai.com/article/\\"}]}"],["meta",{"property":"og:url","content":"https://www.wenzhihuai.com/java/JVM/%E4%B8%80%E6%AC%A1jvm%E8%B0%83%E4%BC%98%E8%BF%87%E7%A8%8B.html"}],["meta",{"property":"og:site_name","content":"个人博客"}],["meta",{"property":"og:title","content":"一次jvm调优过程"}],["meta",{"property":"og:description","content":"一次jvm调优过程 前端时间把公司的一个分布式定时调度的系统弄上了容器云，部署在kubernetes，在容器运行的动不动就出现问题，特别容易jvm溢出，导致程序不可用，终端无法进入，日志一直在刷错误，kubernetes也没有将该容器自动重启。业务方基本每天都在反馈task不稳定，后续就协助接手看了下，先主要讲下该程序的架构吧。 该程序task主要分为..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github-images.wenzhihuai.com/images/20200121102100646998927.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-11-01T04:49:29.000Z"}],["meta",{"property":"article:modified_time","content":"2025-11-01T04:49:29.000Z"}]]},"git":{"createdTime":1579573736000,"updatedTime":1761972569000,"contributors":[{"name":"wenzhihuai","username":"wenzhihuai","email":"1570631036@qq.com","commits":2,"url":"https://github.com/wenzhihuai"},{"name":"zhihuaiwen","username":"zhihuaiwen","email":"zhihuaiwen@tencent.com","commits":5,"url":"https://github.com/zhihuaiwen"}]},"readingTime":{"minutes":6.28,"words":1884},"filePathRelative":"java/JVM/一次jvm调优过程.md","excerpt":"\\n<p>前端时间把公司的一个分布式定时调度的系统弄上了容器云，部署在kubernetes，在容器运行的动不动就出现问题，特别容易jvm溢出，导致程序不可用，终端无法进入，日志一直在刷错误，kubernetes也没有将该容器自动重启。业务方基本每天都在反馈task不稳定，后续就协助接手看了下，先主要讲下该程序的架构吧。\\n该程序task主要分为三个模块：\\nconsole进行一些cron的配置（表达式、任务名称、任务组等）；\\nschedule主要从数据库中读取配置然后装载到quartz再然后进行命令下发；\\nclient接收任务执行，然后向schedule返回运行的信息（成功、失败原因等）。\\n整体架构跟github上开源的xxl-job类似，也可以参考一下。</p>","autoDesc":true}');export{r as comp,g as data};
